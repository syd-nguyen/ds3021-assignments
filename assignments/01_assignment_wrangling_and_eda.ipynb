{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f92d714",
   "metadata": {},
   "source": [
    "# Assignment 1: Wrangling and EDA\n",
    "### Foundations of Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60989a54",
   "metadata": {},
   "source": [
    "**Q1.** This question provides some practice cleaning variables which have common problems.\n",
    "1. Numeric variable: For `airbnb_NYC.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2666e7",
   "metadata": {},
   "source": [
    "You end up with 0 missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7563fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRBNB = pd.read_csv('../data/airbnb_NYC.csv', encoding='latin1')\n",
    "AIRBNB['Price'] = AIRBNB['Price'].str.replace(',','')\n",
    "AIRBNB['Price'] = AIRBNB['Price'].astype(int)\n",
    "AIRBNB['Price'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa555fe",
   "metadata": {},
   "source": [
    "2. Categorical variable: For the Minnesota police use of for data, `mn_police_use_of_force.csv`, clean the `subject_injury` variable, handling the NA's; this gives a value `Yes` when a person was injured by police, and `No` when no injury occurred. What proportion of the values are missing? Cross-tabulate your cleaned `subject_injury` variable with the `force_type` variable. Are there any patterns regarding when the data are missing? For the remaining missing values, replace the `np.nan/None` values with the label `Missing`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ee447",
   "metadata": {},
   "source": [
    "76% of `subject_injury` values are missing. \"Firearm\" and \"Less lethal projectile\" `force_type`s always have a corresponding non-nan `subject_injury` value. \"Baton\" has 2 nan values and (the least for a single `force_type`), and \"Bodily force\" has 7051 nan values (the most for a single `force_type`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNPOLICE = pd.read_csv('../data/mn_police_use_of_force.csv')\n",
    "MNPOLICE['subject_injury'].isna().sum() / MNPOLICE.shape[0] # proportion missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250356c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the force types that are never nan\n",
    "forceTypesWhenNan = MNPOLICE[MNPOLICE['subject_injury'].isna()]['force_type'].unique()\n",
    "allForceTypes = MNPOLICE['force_type'].unique()\n",
    "for forceType in allForceTypes:\n",
    "    if forceType not in forceTypesWhenNan: print(forceType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nas = MNPOLICE[MNPOLICE['subject_injury'].isna()]\n",
    "for forceType in allForceTypes:\n",
    "    print(forceType, '—', nas[nas['force_type'] == forceType].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNPOLICE = MNPOLICE.fillna({'subject_injury': 'Missing'})\n",
    "MNPOLICE['subject_injury'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046f1d9",
   "metadata": {},
   "source": [
    "3. Dummy variable: For `metabric.csv`, convert the `Overall Survival Status` variable into a dummy/binary variable, taking the value 0 if the patient is deceased and 1 if they are living."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2892fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "METABRIC = pd.read_csv('../data/metabric.csv')\n",
    "METABRIC['Overall Survival Status'] = METABRIC['Overall Survival Status'].replace('0:LIVING', '0')\n",
    "METABRIC['Overall Survival Status'] = METABRIC['Overall Survival Status'].replace('1:DECEASED', '1')\n",
    "METABRIC['Overall Survival Status'] = METABRIC['Overall Survival Status'].astype(int)\n",
    "METABRIC['Overall Survival Status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7310b",
   "metadata": {},
   "source": [
    "4. Missing values: For `airbnb_NYC.csv`, determine how many missing values of `Review Scores Rating` there are. Create a new variable, in which you impute the median score for non-missing observations to the missing ones. Why might this bias or otherwise negatively impact your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d2693",
   "metadata": {},
   "source": [
    "There are 8323 missing values of `Review Scores Rating`. This might impact my results because it might under or overrepresent the review scores of certain places. Imputing missing values as the median means that many properties are going to be regarded as just average. However, in acutuality, a reasonable amount of those are probably better than average and a reasonable amount are worse than average. In other words, imputing the median could lead to bias towards the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2693584",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRBNB['Review Scores Rating'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f958db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRBNB2 = AIRBNB.copy()\n",
    "AIRBNB2 = AIRBNB2.fillna({'Review Scores Rating': AIRBNB2['Review Scores Rating'].median()})\n",
    "AIRBNB2['Review Scores Rating'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a66bd",
   "metadata": {},
   "source": [
    "**Q2.** Go to https://sharkattackfile.net/ and download their dataset on shark attacks.\n",
    "\n",
    "1. Open the shark attack file using Pandas. It is probably not a csv file, so `read_csv` won't work. What does work?\n",
    "2. Drop any columns that do not contain data.\n",
    "3. What is an observation? Carefully justify your answer, and explain how it affects your choices in cleaning and analyzing the data.\n",
    "4. Clean the year variable. Describe the range of values you see. Filter the rows to focus on attacks since 1940. Are attacks increasing, decreasing, or remaining constant over time?\n",
    "5. Clean the Age variable and make a histogram of the ages of the victims.\n",
    "6. Clean the `Type` variable so it only takes three values: Provoked and Unprovoked and Unknown. What proportion of attacks are unprovoked?\n",
    "7. Clean the `Fatal Y/N` variable so it only takes three values: Y, N, and Unknown.\n",
    "8. Is the attack more or less likely to be fatal when the attack is provoked or unprovoked? Thoughts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "# read_execel works\n",
    "SHARKS = pd.read_excel('../data/GSAF5.xls')\n",
    "\n",
    "#2\n",
    "SHARKS.dropna(axis='columns', how='all', inplace=True)\n",
    "SHARKS = SHARKS.drop(columns=['Unnamed: 21', 'Unnamed: 22'])\n",
    "\n",
    "SHARKS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b34d4",
   "metadata": {},
   "source": [
    "3. Each observation is one shark attack. It includes info about the location, the type of shark, the activity of the person attacked, etc. This affects my choices in cleaning the data because it means that I shouldn't drop too many rows (eg. for having missing values). I don't want to lose too many shark attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaffc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "# code to find weird things\n",
    "SHARKS['Year'].unique() # weird years are 77, 5, and 0\n",
    "yearIsZero = SHARKS[SHARKS['Year'] == 0]\n",
    "yearIsZero = yearIsZero[yearIsZero['Date'].str.contains('B')]\n",
    "yearIsZero = yearIsZero[yearIsZero['Date'].str.contains('C')]\n",
    "yearIsZero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd1e0c0",
   "metadata": {},
   "source": [
    "The range of values is from 2026 all the way back to 5. It also includes some incidents from BC years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4dfe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to fix weird things\n",
    "\n",
    "# 77 is acceptable, it is for an attack in 77 AD\n",
    "# 5 is also acceptable, it is for an attack in 5 AD\n",
    "\n",
    "# 0 is for a bunch of things :D no dates, ranges, questions, etc.\n",
    "# for simplicity, I change all these into nan values\n",
    "SHARKS['Year'] = SHARKS['Year'].replace(0, np.nan)\n",
    "SHARKS['Year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows to only include years > 1940\n",
    "SHARKS2 = SHARKS.copy()\n",
    "SHARKS2 = SHARKS[SHARKS['Year'] >= 1940]\n",
    "\n",
    "# plot to see trend over time\n",
    "counts = SHARKS2['Year'].value_counts().to_frame()\n",
    "sns.catplot(counts, kind='bar', x='Year', y='count', aspect=3)\n",
    "plt.xticks(rotation=90)\n",
    "plt.suptitle(\"Sharks Attacks Over Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8cfa6",
   "metadata": {},
   "source": [
    "Sharks attacks have generally increased since 1940."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "\n",
    "# code to find and fix weird things\n",
    "# anything that is weird becomes nan for simplicity\n",
    "agesInData = SHARKS2['Age'].unique()\n",
    "weirdAges = []\n",
    "for age in agesInData:\n",
    "    if not str(age).isdigit():\n",
    "        weirdAges.append(age)\n",
    "\n",
    "SHARKS3 = SHARKS2.copy()\n",
    "SHARKS3['Age'] = SHARKS3['Age'].replace(weirdAges, np.nan)\n",
    "SHARKS3['Age'] = SHARKS3['Age'].astype(float)\n",
    "SHARKS3['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4809e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot as histogram\n",
    "\n",
    "sns.catplot(SHARKS3.sort_values('Age'), x='Age', kind='count', aspect=3)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('histogram of ages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "\n",
    "# code to find weird things\n",
    "SHARKS4 = SHARKS3.copy()\n",
    "SHARKS4['Type'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778bf71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to fix weird things\n",
    "toUnknown = ['Questionable', 'Watercraft', 'Sea Disaster', np.nan, '?', 'Unconfirmed', 'Unverified', 'Invalid', 'Under investigation', 'Boat']\n",
    "SHARKS4['Type'] = SHARKS4['Type'].replace(toUnknown, 'Unknown')\n",
    "SHARKS4['Type'] = SHARKS4['Type'].replace('unprovoked', 'Unprovoked')\n",
    "SHARKS4['Type'] = SHARKS4['Type'].replace(' Provoked', 'Provoked')\n",
    "SHARKS4['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find proportion unprovoked\n",
    "len(SHARKS4[SHARKS4['Type'] == 'Unprovoked']) / len(SHARKS4['Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c247a",
   "metadata": {},
   "source": [
    "About 74% of the shark attacks were unprovoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 \n",
    "\n",
    "# code to find weird things\n",
    "SHARKS5 = SHARKS4.copy()\n",
    "SHARKS5['Fatal Y/N'] = SHARKS5['Fatal Y/N'].str.strip()\n",
    "SHARKS5['Fatal Y/N'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to fix weird things\n",
    "toY = ['F', 'Y x 2']\n",
    "toN = ['n', 'Nq']\n",
    "toUnknown2 = ['M', np.nan, 'UNKNOWN']\n",
    "SHARKS5['Fatal Y/N'] = SHARKS5['Fatal Y/N'].replace(toY, 'Y')\n",
    "SHARKS5['Fatal Y/N'] = SHARKS5['Fatal Y/N'].replace(toN, 'N')\n",
    "SHARKS5['Fatal Y/N'] = SHARKS5['Fatal Y/N'].replace(toUnknown2, 'Unknown')\n",
    "SHARKS5['Fatal Y/N'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5029894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "\n",
    "types = SHARKS5['Type'].unique()\n",
    "\n",
    "fatalityByType = pd.DataFrame({\n",
    "    'type': types,\n",
    "    'percentFatal': [0.0, 0.0, 0.0]\n",
    "})\n",
    "fatalityByType = fatalityByType.set_index('type')\n",
    "\n",
    "for type in types:\n",
    "    fatalColumn = SHARKS5[SHARKS5['Type'] == type]['Fatal Y/N']\n",
    "    percentFatal = ( len(fatalColumn[fatalColumn == 'Y']) / len(fatalColumn) ) * 100\n",
    "    fatalityByType.loc[type, 'percentFatal'] = percentFatal\n",
    "    \n",
    "fatalityByType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af244d7",
   "metadata": {},
   "source": [
    "Unprovoked attacks are more likely to be fatal than provoked attacks. This is the opposite of what I would have suspected. Maybe this is because some of the unknown attacks should have been marked as provoked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54a370",
   "metadata": {},
   "source": [
    "**Q3.** Open the \"tidy_data.pdf\" document available in `https://github.com/ds4e/wrangling`, which is a paper called *Tidy Data* by Hadley Wickham.\n",
    "\n",
    "  1. Read the abstract. What is this paper about?\n",
    "\n",
    "  This paper is about the concept of \"tidy data\" which is data where each variable is a column, each observation is a row, and each type of observational unit is a table. Tidy data makes it easier to work with data.\n",
    "\n",
    "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
    "\n",
    "  The \"tidy data standard\" is intended to facilitate initial data exploration and analysis and to simplify th development of data analysis tools that work together. It aims to make data analysis easier and draw focus away from uninteresting logistics.\n",
    "\n",
    "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
    "\n",
    "  The first sentence (\"Like families...\") means that all nice, tidy datasets are standardized and work together. It also means that messy datasets are all varied and structured in different ways. Tidy datasets give structure to messy datasets.\n",
    "\n",
    "  The second sentence (\"For a given dataset...\") means that what should be a variable and what should be an observation can vary by dataset. Wickham gives an example: in one dataset, there could be a `height` variable and a `width` variable; but in another dataset, there could be a `dimension` variable with possible values of `height` and `width`.\n",
    "\n",
    "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
    "\n",
    "  Values are defined as numbers or strings (usually) that each belong to a variable and an observation. Variables are groupings of values that all measure the same attribute across units, for example, height. And observations are groupings of values that all measure the same unit across attributes, for example, people.\n",
    "\n",
    "  5. How is \"Tidy Data\" defined in section 2.3?\n",
    "\n",
    "  \"Tidy Data\" is defined by three characteristics: 1) each variable must form a column, 2) each observation must form a row, and 3) each type of observational unit must form a table.\n",
    "\n",
    "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
    "\n",
    "  The five most common problems with messy datasets are 1) columns are values instead of variables, 2) multiple variables are in one column, 3) variables are in both rows and columns, 4) multiple types of observational units are in the same table, and 5) a single observational unit is in multiple tables.\n",
    "\n",
    "  The data in Table 4 are messy because the values for the variable `income` are in the columns.\n",
    "\n",
    "  \"Melting\" a dataset is taking the columns that are not already variables and turning them into variables. You do this by making a new variable called `column` that contains repeated column headings and by making a new variable `value` with the concatenated data values from the previously separate columns.\n",
    "\n",
    "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
    "\n",
    "  Table 11 is messy because variables are stored in both columns and rows. For example, the variables `id`, `year`, and `month` are in columns but the variables `tmin` and `tmax` are in rows. Table 12 is tidy and molten because it follows the aforementioned three precepts of Tidy Data. The data in Table 12 becomes molten as the variable names are put into an `element` column, and it becomes tidy as the row variables `tmin` and `tmax` are turned into columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58495aa",
   "metadata": {},
   "source": [
    "**Q4.** This question looks at financial transfers from international actors to American universities. In particular, from which countries and giftors are the gifts coming from, and to which institutions are they going? \n",
    "\n",
    "For this question, `.groupby([vars]).count()` and `.groupby([vars]).sum()` will be especially useful to tally the number of occurrences and sum the values of those occurrences.\n",
    "\n",
    "1. Load the `ForeignGifts_edu.csv` dataset.\n",
    "2. For `Foreign Gift Amount`, create a histogram and describe the variable. Describe your findings.\n",
    "3. For `Gift Type`, create a histogram or value counts table. What proportion of the gifts are contracts, real estate, and monetary gifts?\n",
    "4. What are the top 15 countries in terms of the number of gifts? What are the top 15 countries in terms of the amount given?\n",
    "5. What are the top 15 institutions in terms of the total amount of money they receive? Make a histogram of the total amount received by all institutions. \n",
    "6. Which giftors provide the most money, in total? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "GIFTS = pd.read_csv('../data/ForeignGifts_edu.csv')\n",
    "GIFTS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64024dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "sns.histplot(GIFTS['Foreign Gift Amount'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(GIFTS['Foreign Gift Amount']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d57185",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GIFTS['Foreign Gift Amount'].min())\n",
    "print(GIFTS['Foreign Gift Amount'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f29350",
   "metadata": {},
   "source": [
    "The histogram has a long tail, which means that most gifts are not too much money and that few gifts are actually reaching extraoridinary amounts. The `Foreign Gift Amount` variable has a wide range from -537770 all the way to 99999999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "typeCounts = GIFTS['Gift Type'].value_counts().to_frame()\n",
    "totalCount = len(GIFTS['Gift Type'])\n",
    "typeCounts['proportion'] = typeCounts['count'] / totalCount\n",
    "typeCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa1733",
   "metadata": {},
   "source": [
    "About 61% of gifts are contracts, about 39% of gifts are monetary, and about 0.03% of gifts are real estate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2113e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "giftsByCountry = GIFTS.groupby(['Country of Giftor'])['ID'].count().to_frame()\n",
    "giftsByCountry.columns = ['number of gifts']\n",
    "giftsByCountry.sort_values('number of gifts', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "giftAmtsByCountry = GIFTS.groupby(['Country of Giftor'])['Foreign Gift Amount'].sum().to_frame()\n",
    "giftAmtsByCountry.columns = ['amount of gifts']\n",
    "giftAmtsByCountry.sort_values('amount of gifts', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 \n",
    "giftsReceivedByInstitutions = GIFTS.groupby(['Institution Name'])['Foreign Gift Amount'].sum().to_frame()\n",
    "giftsReceivedByInstitutions.columns = ['amount received']\n",
    "giftsReceivedByInstitutions.sort_values('amount received', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(giftsReceivedByInstitutions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbdb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "giftsGivenByGiftor = GIFTS.groupby(['Giftor Name'])['Foreign Gift Amount'].sum().to_frame()\n",
    "giftsGivenByGiftor.columns = ['amount given']\n",
    "giftsGivenByGiftor.sort_values('amount given', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a8e97",
   "metadata": {},
   "source": [
    "**Q5.** This question provides some practice doing exploratory data analysis and visualization.\n",
    "\n",
    "We'll use the `college_completion.csv` dataset from the US Department of Education. The \"relevant\" variables for this question are:\n",
    "  - `level` - Level of institution (4-year, 2-year)\n",
    "  - `aid_value` - The average amount of student aid going to undergraduate recipients\n",
    "  - `control` - Public, Private not-for-profit, Private for-profit\n",
    "  - `grad_100_value` - percentage of first-time, full-time, degree-seeking undergraduates who complete a degree or certificate program within 100 percent of expected time (bachelor's-seeking group at 4-year institutions)\n",
    "\n",
    "1. Load the `college_completion.csv` data with Pandas.\n",
    "2. How many observations and variables are in the data? Use `.head()` to examine the first few rows of data.\n",
    "3. Cross tabulate `control` and `level`. Describe the patterns you see in words.\n",
    "4. For `grad_100_value`, create a kernel density plot and describe table. Now condition on `control`, and produce a kernel density plot and describe tables for each type of institutional control. Which type of institution appear to have the most favorable graduation rates?\n",
    "5. Make a scatterplot of `grad_100_value` by `aid_value`, and compute the covariance and correlation between the two variables. Describe what you see. Now make the same plot and statistics, but conditioning on `control`. Describe what you see. For which kinds of institutions does aid seem to vary positively with graduation rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "COLLEGE = pd.read_csv('../data/college_completion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "COLLEGE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969daaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLEGE.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLEGE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62b684",
   "metadata": {},
   "source": [
    "There are 63 columns in the dataset and thus 63 variables. However, `index` and `unitid` seem to serve more as identifiers rather than actual measured variables. So, there are really 61 variables in the dataset. And, there are 3798 rows in the dataset and thus 3798 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLEGE['control'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "pd.crosstab(COLLEGE['control'], COLLEGE['level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bde07",
   "metadata": {},
   "source": [
    "Most 2-year colleges are public, but most 4-year colleges are private non-profit. An overwhelming majority of private non-profit schools are 4-year. Private for-profit schools are somewhat evenly split between 2-year and 4-year programs, while public colleges tend towards 2-year programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "sns.kdeplot(COLLEGE['grad_100_value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLEGE['grad_100_value'].describe().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(COLLEGE, kind='kde', x='grad_100_value', hue='control')\n",
    "plt.title('100% time completion rate density by control of college')\n",
    "plt.xlabel('100% time completion rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02cf051",
   "metadata": {},
   "outputs": [],
   "source": [
    "for control in COLLEGE['control'].unique():\n",
    "    print(control)\n",
    "    print(COLLEGE[COLLEGE['control'] == control]['grad_100_value'].describe())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb537c",
   "metadata": {},
   "source": [
    "Private non-profit schools seem to have the most favorable graduation rate. Most public schools have a lower graduation rate, as seen by the high peak on the left in the kde plot and the mean of about 17. Continuing to look at the means, we see that private non-protfit schools surpass private for-profit schools with a mean of 41 over a mean of 29. Also, in the kde plot, we see that the private non-profit schools have higher density on the right (than private for-profit schools) which further indicates that provie non-profit schools have the most favorable graduation rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbbaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "sns.scatterplot(COLLEGE, x='aid_value', y='grad_100_value')\n",
    "plt.title('graduation rate by aid received')\n",
    "plt.xlabel('aid received')\n",
    "plt.ylabel('graduation rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLEGE2 = COLLEGE[['grad_100_value', 'aid_value']]\n",
    "COLLEGE2.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLEGE2.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfde0b",
   "metadata": {},
   "source": [
    "The graduation rate and aid amount seem to only somewhat related to one another. In the scatter plot, I see that the graduation rate tends upwards as the aid amount goes up, and this is affirmed by the postive covariance and correlation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(COLLEGE, x='aid_value', y='grad_100_value', hue='control')\n",
    "plt.title('graduation rate by aid received')\n",
    "plt.xlabel('aid received')\n",
    "plt.ylabel('graduation rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for control in COLLEGE['control'].unique():\n",
    "    COLLEGE3 = COLLEGE[COLLEGE['control'] == control][['grad_100_value', 'aid_value']]\n",
    "    print(control)\n",
    "    print(COLLEGE3.cov())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "for control in COLLEGE['control'].unique():\n",
    "    COLLEGE3 = COLLEGE[COLLEGE['control'] == control][['grad_100_value', 'aid_value']]\n",
    "    print(control)\n",
    "    print(COLLEGE3.corr())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d61932",
   "metadata": {},
   "source": [
    "For private non-profit colleges, graduation rate and aid amount seem to be the most strongly related. This is seen in the scatter plot with most of the points that represent higher graduation rates and higher aid amounts being from private non-profit colleges. The covariance and correlation values for private non-profit schools are also higher than that of public schools and private for-profit schools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a468b",
   "metadata": {},
   "source": [
    "**Q6.** In class, we talked about how to compute the sample mean of a variable $X$,\n",
    "$$\n",
    "m(X) = \\dfrac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "and sample covariance of two variables $X$ and $Y$,\n",
    "$$\n",
    "\\text{cov}(X,Y) = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))(y_i - m(Y))).\n",
    "$$\n",
    "Recall, the sample variance of $X$ is\n",
    "$$\n",
    "s^2 = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))^2.\n",
    "$$\n",
    "It can be very helpful to understand some basic properties of these statistics. If you want to write your calculations on a piece of paper, take a photo, and upload that to your GitHub repo, that's probably easiest.\n",
    "\n",
    "We're going to look at **linear transformations** of $X$, $Y = a + bX$. So we take each value of $X$, $x_i$, and transform it as $y_i = a + b x_i$. \n",
    "\n",
    "1. Show that $m(a + bX) = a+b \\times m(X)$.\n",
    "2. Show that $ \\text{cov}(X,X) = s^2$.\n",
    "3. Show that $\\text{cov}(X,a+bY) = b \\times \\text{cov}(X,Y)$\n",
    "4. Show that $\\text{cov}(a+bX,a+bY) = b^2 \\text{cov}(X,Y) $. Notice, this also means that $\\text{cov}(bX, bX) = b^2 s^2$.\n",
    "5. Suppose $b>0$ and let the median of $X$ be $\\text{med}(X)$. Is it true that the median of $a+bX$ is equal to $a + b \\times \\text{med}(X)$? Is the IQR of $a + bX$ equal to $a + b \\times \\text{IQR}(X)$?\n",
    "6. Show by example that the means of $X^2$ and $\\sqrt{X}$ are generally not $(m(X))^2$ and $\\sqrt{m(X)}$. So, the results we derived above really depend on the linearity of the transformation $Y = a + bX$, and transformations like $Y = X^2$ or $Y = \\sqrt{X}$ will not behave in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284bfb7",
   "metadata": {},
   "source": [
    "My answers to these problems are in the same GitHub repo that this notebook is in — also linked [here](https://github.com/syd-nguyen/ds3021-assignments/blob/main/assignments/01_Q6.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f54b39",
   "metadata": {},
   "source": [
    "**Q7.** This question provides some practice doing exploratory data analysis and visualization.\n",
    "\n",
    "We'll use the `ames_prices.csv` dataset. The \"relevant\" variables for this question are:\n",
    "  - `price` - Sale price value of the house\n",
    "  - `Bldg.Type` - Building type of the house (single family home, end-of-unit townhome, duplex, interior townhome, two-family conversion)\n",
    "\n",
    "1. Load the `ames_prices.csv` data with Pandas.\n",
    "2. Make a kernel density plot of price and compute a describe table. Now, make a kernel density plot of price conditional on building type, and use `.groupby()` to make a describe type for each type of building. Which building types are the most expensive, on average? Which have the highest variance in transaction prices?\n",
    "3. Make an ECDF plot of price, and compute the sample minimum, .25 quantile, median, .75 quantile, and sample maximum (i.e. a 5-number summary).\n",
    "4. Make a boxplot of price. Are there outliers? Make a boxplot of price conditional on building type. What patterns do you see?\n",
    "5. Make a dummy variable indicating that an observation is an outlier.\n",
    "6. Winsorize the price variable, and compute a new kernel density plot and describe table. How do the results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "AMES = pd.read_csv('../data/ames_prices.csv')\n",
    "AMES.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "sns.kdeplot(AMES['price'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMES['price'].describe().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a591a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(AMES, kind='kde', x='price', hue='Bldg.Type', common_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1670fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMES.groupby(['Bldg.Type'])['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921a3d3",
   "metadata": {},
   "source": [
    "Townhome end units (`TwnhsE`) are the most expensive on average. 1-family homes have the highest variance (and standard deviation) in price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "sns.ecdfplot(AMES['price'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e113a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveNumSum = pd.DataFrame({\n",
    "    'var': ['min', '25%', 'median', '75%', 'max'],\n",
    "    'value': [\n",
    "        AMES['price'].min(),\n",
    "        np.quantile(AMES['price'], 0.25),\n",
    "        np.quantile(AMES['price'], 0.5),\n",
    "        np.quantile(AMES['price'], 0.75),\n",
    "        AMES['price'].max()\n",
    "    ]\n",
    "})\n",
    "fiveNumSum = fiveNumSum.set_index('var')\n",
    "fiveNumSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83798c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "sns.catplot(AMES, kind='box', x='price', aspect=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628e58a",
   "metadata": {},
   "source": [
    "Yes. There are quite a few outliers on the upper end of price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(AMES, kind='box', x='price', hue='Bldg.Type', aspect=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804ace8",
   "metadata": {},
   "source": [
    "1-family homes have the most outliers on the higher end of price, even though their mean price is within the mean prices of other building types. They also have a wide range of prices. Townhouse end units have a higher mean price than townhouse inside units. The spread of duplex and 2-family conversion homes seems to be smaller than that of other building types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "q25 = np.quantile(AMES['price'], 0.25)\n",
    "q75 = np.quantile(AMES['price'], 0.75)\n",
    "isLowerOutlier = (AMES['price'] < q25).astype(int)\n",
    "isUpperOutlier = (AMES['price'] > q75).astype(int)\n",
    "isOutlier = isLowerOutlier + isUpperOutlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef89f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# 6. Winsorize the price variable, and compute a new kernel density plot and describe table. How do the results change?\n",
    "iqr = q75 - q25\n",
    "uw = q75 + 1.5 * iqr\n",
    "lw = q25 - 1.5 * iqr\n",
    "winsorize = (\n",
    "    isUpperOutlier * uw +\n",
    "    isLowerOutlier * lw +\n",
    "    (1-isOutlier) * AMES['price']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=AMES['price'], y=winsorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f730d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(x=winsorize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f520bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorize.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMES['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f1a0c",
   "metadata": {},
   "source": [
    "There are now three peaks in the kdeplot because of the winsorizing. I think this is because of all the upper outliers and all the lower outliers being set to new values. Those new values are now very dense. Also, the mean has decreased, while the standard deviation has increased. The minimum and the maximum have decreased. The quartiles are the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
