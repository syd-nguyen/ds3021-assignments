{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e5ae83",
   "metadata": {},
   "source": [
    "# Assignment 1: Wrangling and EDA\n",
    "### Foundations of Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079c41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60989a54",
   "metadata": {},
   "source": [
    "**Q1.** This question provides some practice cleaning variables which have common problems.\n",
    "1. Numeric variable: For `airbnb_NYC.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2666e7",
   "metadata": {},
   "source": [
    "You end up with 0 missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7563fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AIRBNB = pd.read_csv('../data/airbnb_NYC.csv', encoding='latin1')\n",
    "AIRBNB['Price'] = AIRBNB['Price'].str.replace(',','')\n",
    "AIRBNB['Price'] = AIRBNB['Price'].astype(int)\n",
    "AIRBNB['Price'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa555fe",
   "metadata": {},
   "source": [
    "2. Categorical variable: For the Minnesota police use of for data, `mn_police_use_of_force.csv`, clean the `subject_injury` variable, handling the NA's; this gives a value `Yes` when a person was injured by police, and `No` when no injury occurred. What proportion of the values are missing? Cross-tabulate your cleaned `subject_injury` variable with the `force_type` variable. Are there any patterns regarding when the data are missing? For the remaining missing values, replace the `np.nan/None` values with the label `Missing`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ee447",
   "metadata": {},
   "source": [
    "76% of `subject_injury` values are missing. \"Firearm\" and \"Less lethal projectile\" `force_type`s always have a corresponding non-nan `subject_injury` value. \"Baton\" has 2 nan values and (the least for a single `force_type`), and \"Bodily force\" has 7051 nan values (the most for a single `force_type`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f9bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7619342359767892)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNPOLICE = pd.read_csv('../data/mn_police_use_of_force.csv')\n",
    "MNPOLICE['subject_injury'].isna().sum() / MNPOLICE.shape[0] # proportion missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250356c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firearm\n",
      "Less Lethal Projectile\n"
     ]
    }
   ],
   "source": [
    "# print the force types that are never nan\n",
    "forceTypesWhenNan = MNPOLICE[MNPOLICE['subject_injury'].isna()]['force_type'].unique()\n",
    "allForceTypes = MNPOLICE['force_type'].unique()\n",
    "for forceType in allForceTypes:\n",
    "    if forceType not in forceTypesWhenNan: print(forceType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edbe6c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bodily Force — 7051\n",
      "Chemical Irritant — 1421\n",
      "Taser — 985\n",
      "Improvised Weapon — 74\n",
      "Gun Point Display — 27\n",
      "Police K9 Bite — 31\n",
      "Baton — 2\n",
      "Firearm — 0\n",
      "Less Lethal Projectile — 0\n",
      "Maximal Restraint Technique — 170\n",
      "Less Lethal — 87\n"
     ]
    }
   ],
   "source": [
    "nas = MNPOLICE[MNPOLICE['subject_injury'].isna()]\n",
    "for forceType in allForceTypes:\n",
    "    print(forceType, '—', nas[nas['force_type'] == forceType].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6f2dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Missing', 'No', 'Yes'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNPOLICE = MNPOLICE.fillna({'subject_injury': 'Missing'})\n",
    "MNPOLICE['subject_injury'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046f1d9",
   "metadata": {},
   "source": [
    "3. Dummy variable: For `metabric.csv`, convert the `Overall Survival Status` variable into a dummy/binary variable, taking the value 0 if the patient is deceased and 1 if they are living."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc2892fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METABRIC = pd.read_csv('../data/metabric.csv')\n",
    "METABRIC['Overall Survival Status'] = METABRIC['Overall Survival Status'].replace('0:LIVING', '0')\n",
    "METABRIC['Overall Survival Status'] = METABRIC['Overall Survival Status'].replace('1:DECEASED', '1')\n",
    "METABRIC['Overall Survival Status'] = METABRIC['Overall Survival Status'].astype(int)\n",
    "METABRIC['Overall Survival Status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7310b",
   "metadata": {},
   "source": [
    "4. Missing values: For `airbnb_NYC.csv`, determine how many missing values of `Review Scores Rating` there are. Create a new variable, in which you impute the median score for non-missing observations to the missing ones. Why might this bias or otherwise negatively impact your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d2693",
   "metadata": {},
   "source": [
    "There are 8323 missing values of `Review Scores Rating`. This might impact my results because it might under or overrepresent the review scores of certain places. Imputing missing values as the median means that many properties are going to be regarded as just average. However, in acutuality, a reasonable amount of those are probably better than average and a reasonable amount are worse than average. In other words, imputing the median could lead to bias towards the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2693584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(8323)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AIRBNB['Review Scores Rating'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f958db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AIRBNB2 = AIRBNB.copy()\n",
    "AIRBNB2 = AIRBNB2.fillna({'Review Scores Rating': AIRBNB2['Review Scores Rating'].median()})\n",
    "AIRBNB2['Review Scores Rating'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a66bd",
   "metadata": {},
   "source": [
    "**Q2.** Go to https://sharkattackfile.net/ and download their dataset on shark attacks.\n",
    "\n",
    "1. Open the shark attack file using Pandas. It is probably not a csv file, so `read_csv` won't work. What does work?\n",
    "2. Drop any columns that do not contain data.\n",
    "3. What is an observation? Carefully justify your answer, and explain how it affects your choices in cleaning and analyzing the data.\n",
    "4. Clean the year variable. Describe the range of values you see. Filter the rows to focus on attacks since 1940. Are attacks increasing, decreasing, or remaining constant over time?\n",
    "5. Clean the Age variable and make a histogram of the ages of the victims.\n",
    "6. Clean the `Type` variable so it only takes three values: Provoked and Unprovoked and Unknown. What proportion of attacks are unprovoked?\n",
    "7. Clean the `Fatal Y/N` variable so it only takes three values: Y, N, and Unknown.\n",
    "8. Is the attack more or less likely to be fatal when the attack is provoked or unprovoked? Thoughts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54a370",
   "metadata": {},
   "source": [
    "**Q3.** Open the \"tidy_data.pdf\" document available in `https://github.com/ds4e/wrangling`, which is a paper called *Tidy Data* by Hadley Wickham.\n",
    "\n",
    "  1. Read the abstract. What is this paper about?\n",
    "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
    "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
    "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
    "  5. How is \"Tidy Data\" defined in section 2.3?\n",
    "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
    "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58495aa",
   "metadata": {},
   "source": [
    "**Q4.** This question looks at financial transfers from international actors to American universities. In particular, from which countries and giftors are the gifts coming from, and to which institutions are they going? \n",
    "\n",
    "For this question, `.groupby([vars]).count()` and `.groupby([vars]).sum()` will be especially useful to tally the number of occurrences and sum the values of those occurrences.\n",
    "\n",
    "1. Load the `ForeignGifts_edu.csv` dataset.\n",
    "2. For `Foreign Gift Amount`, create a histogram and describe the variable. Describe your findings.\n",
    "3. For `Gift Type`, create a histogram or value counts table. What proportion of the gifts are contracts, real estate, and monetary gifts?\n",
    "4. What are the top 15 countries in terms of the number of gifts? What are the top 15 countries in terms of the amount given?\n",
    "5. What are the top 15 institutions in terms of the total amount of money they receive? Make a histogram of the total amount received by all institutions. \n",
    "6. Which giftors provide the most money, in total? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a8e97",
   "metadata": {},
   "source": [
    "**Q5.** This question provides some practice doing exploratory data analysis and visualization.\n",
    "\n",
    "We'll use the `college_completion.csv` dataset from the US Department of Education. The \"relevant\" variables for this question are:\n",
    "  - `level` - Level of institution (4-year, 2-year)\n",
    "  - `aid_value` - The average amount of student aid going to undergraduate recipients\n",
    "  - `control` - Public, Private not-for-profit, Private for-profit\n",
    "  - `grad_100_value` - percentage of first-time, full-time, degree-seeking undergraduates who complete a degree or certificate program within 100 percent of expected time (bachelor's-seeking group at 4-year institutions)\n",
    "\n",
    "1. Load the `college_completion.csv` data with Pandas.\n",
    "2. How many observations and variables are in the data? Use `.head()` to examine the first few rows of data.\n",
    "3. Cross tabulate `control` and `level`. Describe the patterns you see in words.\n",
    "4. For `grad_100_value`, create a kernel density plot and describe table. Now condition on `control`, and produce a kernel density plot and describe tables for each type of institutional control. Which type of institution appear to have the most favorable graduation rates?\n",
    "5. Make a scatterplot of `grad_100_value` by `aid_value`, and compute the covariance and correlation between the two variables. Describe what you see. Now make the same plot and statistics, but conditioning on `control`. Describe what you see. For which kinds of institutions does aid seem to vary positively with graduation rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a468b",
   "metadata": {},
   "source": [
    "**Q6.** In class, we talked about how to compute the sample mean of a variable $X$,\n",
    "$$\n",
    "m(X) = \\dfrac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "and sample covariance of two variables $X$ and $Y$,\n",
    "$$\n",
    "\\text{cov}(X,Y) = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))(y_i - m(Y))).\n",
    "$$\n",
    "Recall, the sample variance of $X$ is\n",
    "$$\n",
    "s^2 = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))^2.\n",
    "$$\n",
    "It can be very helpful to understand some basic properties of these statistics. If you want to write your calculations on a piece of paper, take a photo, and upload that to your GitHub repo, that's probably easiest.\n",
    "\n",
    "We're going to look at **linear transformations** of $X$, $Y = a + bX$. So we take each value of $X$, $x_i$, and transform it as $y_i = a + b x_i$. \n",
    "\n",
    "1. Show that $m(a + bX) = a+b \\times m(X)$.\n",
    "2. Show that $ \\text{cov}(X,X) = s^2$.\n",
    "3. Show that $\\text{cov}(X,a+bY) = b \\times \\text{cov}(X,Y)$\n",
    "4. Show that $\\text{cov}(a+bX,a+bY) = b^2 \\text{cov}(X,Y) $. Notice, this also means that $\\text{cov}(bX, bX) = b^2 s^2$.\n",
    "5. Suppose $b>0$ and let the median of $X$ be $\\text{med}(X)$. Is it true that the median of $a+bX$ is equal to $a + b \\times \\text{med}(X)$? Is the IQR of $a + bX$ equal to $a + b \\times \\text{IQR}(X)$?\n",
    "6. Show by example that the means of $X^2$ and $\\sqrt{X}$ are generally not $(m(X))^2$ and $\\sqrt{m(X)}$. So, the results we derived above really depend on the linearity of the transformation $Y = a + bX$, and transformations like $Y = X^2$ or $Y = \\sqrt{X}$ will not behave in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f54b39",
   "metadata": {},
   "source": [
    "**Q7.** This question provides some practice doing exploratory data analysis and visualization.\n",
    "\n",
    "We'll use the `ames_prices.csv` dataset. The \"relevant\" variables for this question are:\n",
    "  - `price` - Sale price value of the house\n",
    "  - `Bldg.Type` - Building type of the house (single family home, end-of-unit townhome, duplex, interior townhome, two-family conversion)\n",
    "\n",
    "1. Load the `college_completion.csv` data with Pandas.\n",
    "2. Make a kernel density plot of price and compute a describe table. Now, make a kernel density plot of price conditional on building type, and use `.groupby()` to make a describe type for each type of building. Which building types are the most expensive, on average? Which have the highest variance in transaction prices?\n",
    "3. Make an ECDF plot of price, and compute the sample minimum, .25 quantile, median, .75 quantile, and sample maximum (i.e. a 5-number summary).\n",
    "4. Make a boxplot of price. Are there outliers? Make a boxplot of price conditional on building type. What patterns do you see?\n",
    "5. Make a dummy variable indicating that an observation is an outlier.\n",
    "6. Winsorize the price variable, and compute a new kernel density plot and describe table. How do the results change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
